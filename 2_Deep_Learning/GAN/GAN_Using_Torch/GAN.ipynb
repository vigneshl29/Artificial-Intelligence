{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters setting\n",
    "batchSize = 64\n",
    "imageSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vignesh.rs/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:219: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "#Transformations\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the weights_init function\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential( \n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True), \n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True), \n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), \n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the generator\n",
    "netG = G()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the discriminator\n",
    "netD = D() \n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the DCGANs\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) \n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 0.8924 Loss_G: 5.0914\n",
      "[0/25][1/782] Loss_D: 1.2871 Loss_G: 5.9992\n",
      "[0/25][2/782] Loss_D: 1.0614 Loss_G: 7.0164\n",
      "[0/25][3/782] Loss_D: 0.8382 Loss_G: 6.2723\n",
      "[0/25][4/782] Loss_D: 0.9930 Loss_G: 7.3442\n",
      "[0/25][5/782] Loss_D: 0.9626 Loss_G: 8.0727\n",
      "[0/25][6/782] Loss_D: 0.9631 Loss_G: 6.4595\n",
      "[0/25][7/782] Loss_D: 1.0233 Loss_G: 9.5538\n",
      "[0/25][8/782] Loss_D: 0.5385 Loss_G: 7.9008\n",
      "[0/25][9/782] Loss_D: 0.8207 Loss_G: 10.5385\n",
      "[0/25][10/782] Loss_D: 0.4601 Loss_G: 8.1550\n",
      "[0/25][11/782] Loss_D: 0.9555 Loss_G: 12.0867\n",
      "[0/25][12/782] Loss_D: 0.4965 Loss_G: 9.0366\n",
      "[0/25][13/782] Loss_D: 1.0556 Loss_G: 11.1626\n",
      "[0/25][14/782] Loss_D: 0.2416 Loss_G: 9.3709\n",
      "[0/25][15/782] Loss_D: 0.7540 Loss_G: 12.8958\n",
      "[0/25][16/782] Loss_D: 0.2390 Loss_G: 9.6433\n",
      "[0/25][17/782] Loss_D: 0.5732 Loss_G: 11.5685\n",
      "[0/25][18/782] Loss_D: 0.4051 Loss_G: 9.3013\n",
      "[0/25][19/782] Loss_D: 1.0162 Loss_G: 16.8347\n",
      "[0/25][20/782] Loss_D: 0.6060 Loss_G: 14.1303\n",
      "[0/25][21/782] Loss_D: 0.1105 Loss_G: 7.2305\n",
      "[0/25][22/782] Loss_D: 3.5223 Loss_G: 18.8404\n",
      "[0/25][23/782] Loss_D: 0.4859 Loss_G: 19.6035\n",
      "[0/25][24/782] Loss_D: 0.2740 Loss_G: 13.8226\n",
      "[0/25][25/782] Loss_D: 0.2017 Loss_G: 5.8832\n",
      "[0/25][26/782] Loss_D: 3.1621 Loss_G: 18.5229\n",
      "[0/25][27/782] Loss_D: 0.3411 Loss_G: 20.3440\n",
      "[0/25][28/782] Loss_D: 0.1614 Loss_G: 16.2889\n",
      "[0/25][29/782] Loss_D: 0.2318 Loss_G: 7.5547\n",
      "[0/25][30/782] Loss_D: 2.5800 Loss_G: 17.4449\n",
      "[0/25][31/782] Loss_D: 0.3449 Loss_G: 18.5383\n",
      "[0/25][32/782] Loss_D: 0.2568 Loss_G: 14.3370\n",
      "[0/25][33/782] Loss_D: 0.1442 Loss_G: 6.4293\n",
      "[0/25][34/782] Loss_D: 3.1338 Loss_G: 19.4468\n",
      "[0/25][35/782] Loss_D: 0.2764 Loss_G: 22.1321\n",
      "[0/25][36/782] Loss_D: 0.1776 Loss_G: 19.6037\n",
      "[0/25][37/782] Loss_D: 0.2375 Loss_G: 13.6178\n",
      "[0/25][38/782] Loss_D: 0.1650 Loss_G: 5.8627\n",
      "[0/25][39/782] Loss_D: 2.2650 Loss_G: 19.3814\n",
      "[0/25][40/782] Loss_D: 0.2336 Loss_G: 22.0920\n",
      "[0/25][41/782] Loss_D: 0.1790 Loss_G: 19.5686\n",
      "[0/25][42/782] Loss_D: 0.1639 Loss_G: 14.0564\n",
      "[0/25][43/782] Loss_D: 0.0513 Loss_G: 6.9559\n",
      "[0/25][44/782] Loss_D: 0.7826 Loss_G: 14.2915\n",
      "[0/25][45/782] Loss_D: 0.1182 Loss_G: 15.1678\n",
      "[0/25][46/782] Loss_D: 0.1294 Loss_G: 11.5604\n",
      "[0/25][47/782] Loss_D: 0.0940 Loss_G: 6.6372\n",
      "[0/25][48/782] Loss_D: 0.6113 Loss_G: 13.0219\n",
      "[0/25][49/782] Loss_D: 0.0523 Loss_G: 13.9912\n",
      "[0/25][50/782] Loss_D: 0.1806 Loss_G: 10.1402\n",
      "[0/25][51/782] Loss_D: 0.2233 Loss_G: 6.0046\n",
      "[0/25][52/782] Loss_D: 2.0641 Loss_G: 23.8006\n",
      "[0/25][53/782] Loss_D: 0.5949 Loss_G: 27.6237\n",
      "[0/25][54/782] Loss_D: 0.6930 Loss_G: 25.3542\n",
      "[0/25][55/782] Loss_D: 0.1415 Loss_G: 18.8837\n",
      "[0/25][56/782] Loss_D: 0.0306 Loss_G: 8.0790\n",
      "[0/25][57/782] Loss_D: 2.7622 Loss_G: 22.9346\n",
      "[0/25][58/782] Loss_D: 1.2897 Loss_G: 23.9862\n",
      "[0/25][59/782] Loss_D: 0.8875 Loss_G: 19.9699\n",
      "[0/25][60/782] Loss_D: 0.1009 Loss_G: 14.1966\n",
      "[0/25][61/782] Loss_D: 0.0293 Loss_G: 6.8425\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.data, errG.data))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./gan_results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./gan_results\", epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
